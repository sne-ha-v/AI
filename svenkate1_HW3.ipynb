{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AYBvZS0vQ-gr"
      },
      "source": [
        "# CS 541-A-Homework 3\n",
        "## Neural networks\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "### ***Fill your details below***\n",
        "### Name: Sneha Venkatesh\n",
        "### CWID: 20027527\n",
        "### Email ID: svenkate1@stevens.edu\n",
        "### References: ***Cite your references here***\n",
        "\n",
        "\n",
        "---\n",
        "### Submission guidelines:\n",
        "\n",
        "#### 1. Submit this notebook along with its PDF version. You can do this by clicking File->Print->\"Save as PDF\"\n",
        "\n",
        "#### 2. Name the file as \"<mailID_HWnumber.extension>\". For example, mailID is abcdefg @stevens.edu then name the files as abcdefg_HW1.ipynb and abcdefg_HW1.pdf.\n",
        "\n",
        "#### 3. Please do not Zip your files.\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PtKvmZx-WmUu",
        "outputId": "daee7a21-89eb-4184-9486-3298b5208f8e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (2.2.0)\n",
            "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch) (4.9.0)\n",
            "Requirement already satisfied: sympy in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch) (2023.10.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: torchvision in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (0.17.0)\n",
            "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torchvision) (2.31.0)\n",
            "Requirement already satisfied: torch==2.2.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torchvision) (2.2.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torchvision) (10.2.0)\n",
            "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch==2.2.0->torchvision) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch==2.2.0->torchvision) (4.9.0)\n",
            "Requirement already satisfied: sympy in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch==2.2.0->torchvision) (1.12)\n",
            "Requirement already satisfied: networkx in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch==2.2.0->torchvision) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch==2.2.0->torchvision) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch==2.2.0->torchvision) (2023.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests->torchvision) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests->torchvision) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests->torchvision) (2.2.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests->torchvision) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from jinja2->torch==2.2.0->torchvision) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from sympy->torch==2.2.0->torchvision) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "#@title Installing Pytorch\n",
        "\n",
        "!pip install torch\n",
        "!pip install torchvision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "bGU6NwlsXFSt"
      },
      "outputs": [],
      "source": [
        "#@title Import Dependencies\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.datasets as dsets\n",
        "import torchvision.transforms as transforms\n",
        "from torch.autograd import Variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "_bNfVLRUYqZA"
      },
      "outputs": [],
      "source": [
        "#@title Define Hyperparameters\n",
        "\n",
        "input_size = 784 # img_size = (28,28) ---> 28*28=784 in total\n",
        "hidden_size = 500 # number of nodes at hidden layer\n",
        "num_classes = 10 # number of output classes discrete range [0,9]\n",
        "num_epochs = 5 # number of times which the entire dataset is passed throughout the model\n",
        "batch_size = 100 # the size of input data took for one iteration\n",
        "lr = 1e-3 # size of step"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lCsBCXMwbpH5",
        "outputId": "63b32008-7b11-4e27-8b2c-103a4e53b604"
      },
      "outputs": [],
      "source": [
        "#@title Downloading MNIST data\n",
        "\n",
        "train_data = dsets.MNIST(root = './data', train = True,\n",
        "                        transform = transforms.ToTensor(), download = True)\n",
        "\n",
        "test_data = dsets.MNIST(root = './data', train = False,\n",
        "                       transform = transforms.ToTensor())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "rfDPBdnYgfGp"
      },
      "outputs": [],
      "source": [
        "#@title Loading the data\n",
        "\n",
        "train_gen = torch.utils.data.DataLoader(dataset = train_data,\n",
        "                                             batch_size = batch_size,\n",
        "                                             shuffle = True)\n",
        "\n",
        "test_gen = torch.utils.data.DataLoader(dataset = test_data,\n",
        "                                      batch_size = batch_size,\n",
        "                                      shuffle = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 421
        },
        "id": "COAe9-DrW59d",
        "outputId": "896de66f-a6b5-4fac-d124-1bafa1816b75"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First 5 images in train dataset:\n"
          ]
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAxsAAACvCAYAAACVbcM3AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/H5lhTAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAbyUlEQVR4nO3de1SVVf7H8e9REfCCjIpalqh5y8lbXocxL4lZXgqTNMtbOebKG8uljqNjysykecMUb7l0eSFdi1wqajZNNiNWloOS6SwyjLxEGMtAA8Qbw/D8/pifTs/ZWzkezuZwDu/XWv6xP+7znK+0A7487Gc7LMuyBAAAAAA8rIq3CwAAAADgn2g2AAAAABhBswEAAADACJoNAAAAAEbQbAAAAAAwgmYDAAAAgBE0GwAAAACMoNkAAAAAYATNBgAAAAAjKn2zceHCBXE4HLJ8+XKPXfPw4cPicDjk8OHDHrsm/BPrD97E+oO3sQbhTay/8uGTzcbWrVvF4XBIamqqt0sxIjY2VhwOh/InKCjI26VB/H/9iYhcvHhRhg8fLqGhoRISEiLPPfecnDt3zttlQSrH+vul/v37i8PhkClTpni7FPw/f1+DZ86ckenTp0tERIQEBQWJw+GQCxcueLss/D9/X38iIomJifL4449LUFCQhIWFyfjx4yU3N9fbZbmtmrcLwN2tX79eatWqdWdctWpVL1aDyqKwsFD69u0r+fn5MnfuXAkICJC3335bevfuLSdPnpR69ep5u0RUEnv27JGjR496uwxUMkePHpX4+Hhp27atPProo3Ly5Elvl4RKZP369TJp0iTp16+frFixQrKysmTVqlWSmpoqKSkpPvmDZ5qNCiw6Olrq16/v7TJQyaxbt04yMjLk2LFj0rVrVxEReeaZZ+Sxxx6TuLg4WbRokZcrRGVw8+ZNmTFjhsyePVvmz5/v7XJQiTz77LOSl5cntWvXluXLl9NsoNwUFRXJ3LlzpVevXvLxxx+Lw+EQEZGIiAgZMmSIbNy4UaZOnerlKu+fT/4alSuKiopk/vz50rlzZ6lTp47UrFlTnnjiCUlOTr7ra95++20JDw+X4OBg6d27t6SlpSlz0tPTJTo6WurWrStBQUHSpUsX2b9/f6n1XL9+XdLT0+/rNphlWVJQUCCWZbn8GlQMvrz+du3aJV27dr3TaIiItGnTRvr16yc7d+4s9fXwPl9ef7ctXbpUSkpKZObMmS6/BhWHL6/BunXrSu3atUudh4rLV9dfWlqa5OXlyYgRI+40GiIigwcPllq1akliYmKp71UR+W2zUVBQIJs2bZI+ffrIkiVLJDY2VnJycmTAgAHan1IkJCRIfHy8TJ48WebMmSNpaWny5JNPyqVLl+7M+frrr6VHjx7yzTffyB/+8AeJi4uTmjVrSlRUlCQlJd2znmPHjsmjjz4qa9ascfnf0Lx5c6lTp47Url1bRo0aZasFFZuvrr+SkhL517/+JV26dFH+rlu3bnL27Fm5evWqax8EeI2vrr/bMjMzZfHixbJkyRIJDg6+r387KgZfX4Pwbb66/m7duiUiov28FxwcLF999ZWUlJS48BGoYCwftGXLFktErOPHj991TnFxsXXr1i1b9vPPP1sNGza0Xn311TvZ+fPnLRGxgoODraysrDt5SkqKJSLW9OnT72T9+vWz2rVrZ928efNOVlJSYkVERFgtW7a8kyUnJ1siYiUnJyvZggULSv33rVy50poyZYq1Y8cOa9euXVZMTIxVrVo1q2XLllZ+fn6pr4dZ/rz+cnJyLBGx/vznPyt/t3btWktErPT09HteA2b58/q7LTo62oqIiLgzFhFr8uTJLr0W5lWGNXjbsmXLLBGxzp8/f1+vgzn+vP5ycnIsh8NhjR8/3panp6dbImKJiJWbm3vPa1REfntno2rVqlK9enUR+e9Pa69cuSLFxcXSpUsXOXHihDI/KipKGjdufGfcrVs36d69u/z1r38VEZErV67IoUOHZPjw4XL16lXJzc2V3NxcuXz5sgwYMEAyMjLk4sWLd62nT58+YlmWxMbGllp7TEyMrF69Wl566SUZNmyYrFy5UrZt2yYZGRmybt26+/xIwBt8df3duHFDREQCAwOVv7u9Ke32HFRcvrr+RESSk5Nl9+7dsnLlyvv7R6NC8eU1CN/nq+uvfv36Mnz4cNm2bZvExcXJuXPn5LPPPpMRI0ZIQECAiPjm12C/bTZERLZt2ybt27eXoKAgqVevnoSFhckHH3wg+fn5ytyWLVsqWatWre487u67774Ty7LkjTfekLCwMNufBQsWiIjITz/9ZOzf8tJLL0mjRo3k73//u7H3gGf54vq7fev29q3cX7p586ZtDio2X1x/xcXFMm3aNBk9erRtzxB8ky+uQfgPX11/GzZskIEDB8rMmTPlkUcekV69ekm7du1kyJAhIiK2p5T6Cr99GtX27dtl3LhxEhUVJbNmzZIGDRpI1apV5a233pKzZ8/e9/Vu/47czJkzZcCAAdo5LVq0KFPNpXn44YflypUrRt8DnuGr669u3boSGBgo2dnZyt/dzh588MEyvw/M8tX1l5CQIGfOnJENGzYo5xpcvXpVLly4IA0aNJAaNWqU+b1glq+uQfgHX15/derUkX379klmZqZcuHBBwsPDJTw8XCIiIiQsLExCQ0M98j7lyW+bjV27dknz5s1lz549th39tztQZxkZGUr27bffStOmTUXkv5u1RUQCAgIkMjLS8wWXwrIsuXDhgnTq1Knc3xv3z1fXX5UqVaRdu3baw5JSUlKkefPmPKXFB/jq+svMzJR///vf8tvf/lb5u4SEBElISJCkpCSJiooyVgM8w1fXIPyDP6y/Jk2aSJMmTUREJC8vT7788ksZNmxYuby3p/ntr1HdPgDP+sVjY1NSUu56QNTevXttv2937NgxSUlJkWeeeUZERBo0aCB9+vSRDRs2aH/qm5OTc8967uexe7prrV+/XnJycuTpp58u9fXwPl9ef9HR0XL8+HFbw3HmzBk5dOiQvPDCC6W+Ht7nq+vvxRdflKSkJOWPiMjAgQMlKSlJunfvfs9roGLw1TUI/+Bv62/OnDlSXFws06dPd+v13ubTdzY2b94sf/vb35Q8JiZGBg8eLHv27JGhQ4fKoEGD5Pz58/LOO+9I27ZtpbCwUHlNixYtpGfPnvL666/LrVu3ZOXKlVKvXj35/e9/f2fO2rVrpWfPntKuXTuZMGGCNG/eXC5duiRHjx6VrKwsOXXq1F1rPXbsmPTt21cWLFhQ6gah8PBwGTFihLRr106CgoLkyJEjkpiYKB07dpSJEye6/gGCUf66/iZNmiQbN26UQYMGycyZMyUgIEBWrFghDRs2lBkzZrj+AYJR/rj+2rRpI23atNH+XbNmzbijUcH44xoUEcnPz5fVq1eLiMjnn38uIiJr1qyR0NBQCQ0NlSlTprjy4YFh/rr+Fi9eLGlpadK9e3epVq2a7N27Vw4ePChvvvmm7+5lK/8HYJXd7cee3e3PDz/8YJWUlFiLFi2ywsPDrcDAQKtTp07WgQMHrLFjx1rh4eF3rnX7sWfLli2z4uLirIcfftgKDAy0nnjiCevUqVPKe589e9YaM2aM1ahRIysgIMBq3LixNXjwYGvXrl135pT1sXu/+93vrLZt21q1a9e2AgICrBYtWlizZ8+2CgoKyvJhg4f4+/qzLMv64YcfrOjoaCskJMSqVauWNXjwYCsjI8PdDxk8qDKsP2fCo28rFH9fg7dr0v35Ze3wDn9ffwcOHLC6detm1a5d26pRo4bVo0cPa+fOnWX5kHmdw7I4nhoAAACA5/ntng0AAAAA3kWzAQAAAMAImg0AAAAARtBsAAAAADCCZgMAAACAETQbAAAAAIxw+VC/Xx73DtxWXk9OZv1Bpzyf3M0ahA6fA+FNrD94k6vrjzsbAAAAAIyg2QAAAABgBM0GAAAAACNoNgAAAAAYQbMBAAAAwAiaDQAAAABG0GwAAAAAMIJmAwAAAIARNBsAAAAAjKDZAAAAAGAEzQYAAAAAI2g2AAAAABhBswEAAADACJoNAAAAAEbQbAAAAAAwgmYDAAAAgBE0GwAAAACMoNkAAAAAYEQ1bxcAoOw6d+6sZFOmTLGNx4wZo8xJSEhQstWrVyvZiRMnylAdAACorLizAQAAAMAImg0AAAAARtBsAAAAADCCZgMAAACAEQ7LsiyXJjocpmvxuqpVqypZnTp13L6e8wbdGjVqKHNat26tZJMnT1ay5cuX28YjR45U5ty8eVPJFi9erGR/+tOf1GLd5OLyKbPKsP5c1bFjRyU7dOiQkoWEhLh1/fz8fCWrV6+eW9cyrbzWnwhr0Nv69etnG+/YsUOZ07t3byU7c+aMsZpE+Bzo6+bNm6dkuq+RVarYfzbbp08fZc4nn3zisbpcxfqDN7m6/rizAQAAAMAImg0AAAAARtBsAAAAADCCZgMAAACAET5/gniTJk2UrHr16koWERGhZD179rSNQ0NDlTnDhg1zvzgXZGVlKVl8fLySDR061Da+evWqMufUqVNK5o0Na/Ccbt26Kdnu3buVTPcgA+eNW7o1U1RUpGS6zeA9evSwjXUniuuuBb1evXopme7jnpSUVB7l+ISuXbvaxsePH/dSJfBV48aNU7LZs2crWUlJSanXKs+HUwC+jjsbAAAAAIyg2QAAAABgBM0GAAAAACN8as+Gq4eZleUgPpN0vweqO1CosLBQyZwPsMrOzlbm/Pzzz0pm+kAruM/5kMfHH39cmbN9+3Yle+CBB9x6v4yMDCVbunSpkiUmJirZ559/bhvr1u1bb73lVl2Vke5AsJYtWypZZd2z4XyAmohIs2bNbOPw8HBlDgeP4V50ayYoKMgLlaAi6t69u5KNGjVKyXSHh/76178u9fozZ85Ush9//FHJnPcTi6jfC6SkpJT6fhUJdzYAAAAAGEGzAQAAAMAImg0AAAAARtBsAAAAADDCpzaIZ2ZmKtnly5eVzPQGcd3GnLy8PCXr27evbaw79Ozdd9/1WF3wLRs2bLCNR44cafT9dBvQa9WqpWS6gyCdNzS3b9/eY3VVRmPGjFGyo0ePeqGSikn3EIQJEybYxrqHJ6SnpxurCb4nMjLSNp46dapLr9Oto8GDB9vGly5dcr8wVAgjRoywjVetWqXMqV+/vpLpHkRx+PBhJQsLC7ONly1b5lJduus7X+vFF1906VoVBXc2AAAAABhBswEAAADACJoNAAAAAEbQbAAAAAAwwqc2iF+5ckXJZs2apWTOG7lERL766isli4+PL/U9T548qWT9+/dXsmvXrimZ84mSMTExpb4f/FPnzp2VbNCgQbaxq6cf6zZwv//++0q2fPly21h3Uqnu/wvdSfRPPvmkbcxJzWWjOyEb/7Np06ZS52RkZJRDJfAVulOXt2zZYhu7+vAY3Ube77//3r3CUO6qVVO/te3SpYuSbdy40TauUaOGMufTTz9Vsr/85S9KduTIESULDAy0jXfu3KnMeeqpp5RMJzU11aV5FRVf8QAAAAAYQbMBAAAAwAiaDQAAAABG0GwAAAAAMMKnNojr7N27V8kOHTqkZFevXlWyDh062Mbjx49X5jhvshXRbwbX+frrr23j1157zaXXwbd17NhRyT7++GMlCwkJsY0ty1LmfPjhh0qmO2m8d+/eSjZv3jzbWLfpNicnR8lOnTqlZCUlJbax8+Z2Ef0J5SdOnFCyykZ32nrDhg29UInvcGUjr+7/KVReY8eOVbIHH3yw1NfpTn5OSEjwREnwklGjRimZKw+d0H1OcT5lXESkoKDApTqcX+vqZvCsrCwl27Ztm0uvrai4swEAAADACJoNAAAAAEbQbAAAAAAwgmYDAAAAgBE+v0Fcx9XNO/n5+aXOmTBhgpK99957Sua8gRaVQ6tWrZRMd6q9bsNrbm6ubZydna3M0W0KKywsVLIPPvjApcxTgoODlWzGjBlK9vLLLxurwVcMHDhQyXQfv8pKt1m+WbNmpb7u4sWLJsqBD6hfv76Svfrqq0rm/HU5Ly9PmfPmm296rC6UP91p3nPnzlUy3QNY1q1bZxs7P1RFxPXvJ3X++Mc/uvW6adOmKZnuYS6+hDsbAAAAAIyg2QAAAABgBM0GAAAAACP8cs+Gq2JjY23jzp07K3N0h6VFRkYq2cGDBz1WFyqmwMBAJdMd+qj7HX3doZJjxoyxjVNTU5U5vvS7/U2aNPF2CRVS69atXZrnfAhoZaH7f0i3j+Pbb7+1jXX/T8H/NG3aVMl2797t1rVWr16tZMnJyW5dC+Vv/vz5Sqbbn1FUVKRkH330kZLNnj3bNr5x44ZLdQQFBSmZ7sA+56+JDodDmaPbM7Rv3z6X6vAl3NkAAAAAYATNBgAAAAAjaDYAAAAAGEGzAQAAAMCISr1B/Nq1a7ax7gC/EydOKNnGjRuVTLfJzHnD79q1a5U5uoNmUDF16tRJyXSbwXWee+45Jfvkk0/KXBP8x/Hjx71dQpmEhIQo2dNPP20bjxo1Spmj21ip43x4l+6ANvgf5zUkItK+fXuXXvuPf/zDNl61apVHakL5CA0NtY0nTZqkzNF9D6XbDB4VFeVWDS1atFCyHTt2KJnuAUPOdu3apWRLly51qy5fw50NAAAAAEbQbAAAAAAwgmYDAAAAgBE0GwAAAACMqNQbxJ2dPXtWycaNG6dkW7ZsUbLRo0eXmtWsWVOZk5CQoGTZ2dn3KhNesmLFCiXTnQiq2/jt65vBq1Sx/1yipKTES5X4r7p163rsWh06dFAy3VqNjIy0jR966CFlTvXq1ZXs5ZdfVjLnNSKinsibkpKizLl165aSVaumfmn68ssvlQz+RbeJd/HixS699siRI0o2duxY2zg/P9+tuuAdzp976tev79Lrpk2bpmQNGjRQsldeecU2fvbZZ5U5jz32mJLVqlVLyXQb1Z2z7du3K3OcH1Tkr7izAQAAAMAImg0AAAAARtBsAAAAADCCZgMAAACAEWwQL0VSUpKSZWRkKJlu83C/fv1s40WLFilzwsPDlWzhwoVKdvHixXvWCc8bPHiwbdyxY0dljm5T2P79+02V5DXOG8J1/+6TJ0+WUzW+xXmTtIj+4/fOO+8o2dy5c916T90Jy7oN4sXFxbbx9evXlTmnT59Wss2bNytZamqqkjk/GOHSpUvKnKysLCULDg5WsvT0dCWDb2vatKltvHv3brevde7cOSXTrTf4jqKiIts4JydHmRMWFqZk58+fVzLd51xX/Pjjj0pWUFCgZA888ICS5ebm2sbvv/++WzX4A+5sAAAAADCCZgMAAACAETQbAAAAAIyg2QAAAABgBBvE3ZCWlqZkw4cPV7IhQ4bYxrqTxydOnKhkLVu2VLL+/fvfT4nwAOdNqrqTlH/66Scle++994zV5GmBgYFKFhsbW+rrDh06pGRz5szxREl+Z9KkSUr2/fffK1lERITH3jMzM1PJ9u7dq2TffPONbfzPf/7TYzXovPbaa0qm2+Cp2+wL/zN79mzb2PlBFPfD1ZPG4Tvy8vJsY90J8wcOHFCyunXrKtnZs2eVbN++fbbx1q1blTlXrlxRssTERCXTbRDXzausuLMBAAAAwAiaDQAAAABG0GwAAAAAMII9Gx7i/LuFIiLvvvuubbxp0yZlTrVq6n+CXr16KVmfPn1s48OHD99XfTDj1q1bSpadne2FSkqn258xb948JZs1a5aSOR+8FhcXp8wpLCwsQ3WVy5IlS7xdglc4H3R6N2U53A0Vk+5Q1Keeesqtazn/rr2IyJkzZ9y6FnxHSkqKkun2fHmS7vux3r17K5luvxF7z/6HOxsAAAAAjKDZAAAAAGAEzQYAAAAAI2g2AAAAABjBBnE3tG/fXsmio6OVrGvXrraxbjO4zunTp5Xs008/dbE6lKf9+/d7u4S7ct6Qqdv4PWLECCXTbb4cNmyYx+oCSpOUlOTtEuBhBw8eVLJf/epXpb5Od9DkuHHjPFESUCrnw31F9JvBLctSMg71+x/ubAAAAAAwgmYDAAAAgBE0GwAAAACMoNkAAAAAYAQbxH+hdevWSjZlyhQle/7555WsUaNGbr3nf/7zHyXTnUCt25AEsxwOxz3HIiJRUVFKFhMTY6qku5o+fbqSvfHGG7ZxnTp1lDk7duxQsjFjxniuMAAQkXr16imZK1/X1q1bp2SFhYUeqQkozUcffeTtEvwCdzYAAAAAGEGzAQAAAMAImg0AAAAARtBsAAAAADCi0mwQ123gHjlypG2s2wzetGlTj9WQmpqqZAsXLlSyinwqdWXifCKo7oRQ3bqKj49Xss2bNyvZ5cuXbeMePXooc0aPHq1kHTp0ULKHHnpIyTIzM21j3UY33eZLoDzpHrzQqlUrJdOdJI2KacuWLUpWpYp7P9v84osvyloO4LYBAwZ4uwS/wJ0NAAAAAEbQbAAAAAAwgmYDAAAAgBE+v2ejYcOGSta2bVslW7NmjZK1adPGY3WkpKQo2bJly2zjffv2KXM4rM+3Va1aVckmTZqkZMOGDVOygoIC27hly5Zu16H7vebk5GTbeP78+W5fHzBFtxfK3d/vR/nr2LGjkkVGRiqZ7mtdUVGRbbx27VplzqVLl9wvDiij5s2be7sEv8BndAAAAABG0GwAAAAAMIJmAwAAAIARNBsAAAAAjKjQG8Tr1q1rG2/YsEGZo9uc5skNPbqNt3FxcUqmOzDtxo0bHqsD5e/o0aO28fHjx5U5Xbt2delausP/dA83cOZ88J+ISGJiopLFxMS4VAfgC37zm98o2datW8u/EJQqNDRUyXSf73QuXrxoG8+cOdMTJQEe89lnnymZ7gEWPOzn3rizAQAAAMAImg0AAAAARtBsAAAAADCCZgMAAACAEV7ZIN69e3clmzVrlpJ169bNNm7cuLFH67h+/bptHB8fr8xZtGiRkl27ds2jdaBiysrKso2ff/55Zc7EiROVbN68eW6936pVq5Rs/fr1Svbdd9+5dX2gInI4HN4uAQC00tLSlCwjI0PJdA8meuSRR2zjnJwczxXmY7izAQAAAMAImg0AAAAARtBsAAAAADCCZgMAAACAEV7ZID506FCXMlecPn1ayQ4cOKBkxcXFSuZ8EnheXp5bNaByyM7OVrLY2FiXMgAiH374oZK98MILXqgEnpKenq5kX3zxhZL17NmzPMoBjNM9OGjTpk1KtnDhQtt46tSpyhzd97D+iDsbAAAAAIyg2QAAAABgBM0GAAAAACNoNgAAAAAY4bAsy3JpIqe8QsPF5VNmrD/olNf6E2ENQo/PgfAm1l/5CwkJUbKdO3cqWWRkpG28Z88eZc4rr7yiZNeuXStDdeXL1fXHnQ0AAAAARtBsAAAAADCCZgMAAACAEezZQJnw+6LwJvZswNv4HAhvYv1VDLp9HM6H+r3++uvKnPbt2yuZLx30x54NAAAAAF5FswEAAADACJoNAAAAAEbQbAAAAAAwgg3iKBM2p8Gb2CAOb+NzILyJ9QdvYoM4AAAAAK+i2QAAAABgBM0GAAAAACNoNgAAAAAY4fIGcQAAAAC4H9zZAAAAAGAEzQYAAAAAI2g2AAAAABhBswEAAADACJoNAAAAAEbQbAAAAAAwgmYDAAAAgBE0GwAAAACMoNkAAAAAYMT/Af6T9PifD5VrAAAAAElFTkSuQmCC",
            "text/plain": [
              "<Figure size 1000x200 with 5 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "First 5 images in test dataset:\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1000x200 with 5 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "transform = transforms.ToTensor()\n",
        "\n",
        "print(\"First 5 images in train dataset:\")\n",
        "fig, axes = plt.subplots(1, 5, figsize=(10, 2))\n",
        "for i in range(5):\n",
        "    image, label = train_data[i]\n",
        "    axes[i].imshow(image.squeeze().numpy(), cmap='gray')\n",
        "    axes[i].set_title('Label: {}'.format(label))\n",
        "    axes[i].axis('off')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nFirst 5 images in test dataset:\")\n",
        "fig, axes = plt.subplots(1, 5, figsize=(10, 2))\n",
        "for i in range(5):\n",
        "    image, label = test_data[i]\n",
        "    axes[i].imshow(image.squeeze().numpy(), cmap='gray')\n",
        "    axes[i].set_title('Label: {}'.format(label))\n",
        "    axes[i].axis('off')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7z_yHL9bPDI-"
      },
      "source": [
        "## Q1. (50 points): design a neural network, provide justification.\n",
        "\n",
        "PyTorch neural network documentation: https://pytorch.org/docs/stable/nn.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "fL-YXTvghaz_"
      },
      "outputs": [],
      "source": [
        "#@title Define model class\n",
        "\n",
        "class Net(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, num_classes):\n",
        "    super(Net,self).__init__()\n",
        "    ''' code to build the model '''\n",
        "    self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.fc2 = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "\n",
        "  def forward(self,x):\n",
        "    ''' code to define the forward pass '''\n",
        "    out = self.fc1(x)\n",
        "    out = self.relu(out)\n",
        "    output = self.fc2(out)\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "-3EPEqbjjfAT"
      },
      "outputs": [],
      "source": [
        "#@title Build the model\n",
        "\n",
        "net = Net(input_size, hidden_size, num_classes)\n",
        "if torch.cuda.is_available():\n",
        "  net.cuda()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "ePLIwvAFj2zH"
      },
      "outputs": [],
      "source": [
        "#@title Define loss-function & optimizer\n",
        "\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(net.parameters(), lr=lr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r6IZ2UotP39e"
      },
      "source": [
        "##Q2.\n",
        "\n",
        "##a) In the code provided below, What is the meaning of the \"loss.backward()\" step? Please explain the functionality. (15 pts)\n",
        "\n",
        "##b) What is the meaning of \"optimizer.step()\" and what does it do? (15 pts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u75Xa5VckuTH",
        "outputId": "27ede945-0d79-4738-c725-f46f1757902f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/5], Step [100/600], Loss: 0.4093\n",
            "Epoch [1/5], Step [200/600], Loss: 0.2264\n",
            "Epoch [1/5], Step [300/600], Loss: 0.2733\n",
            "Epoch [1/5], Step [400/600], Loss: 0.2714\n",
            "Epoch [1/5], Step [500/600], Loss: 0.1924\n",
            "Epoch [1/5], Step [600/600], Loss: 0.1618\n",
            "Epoch [2/5], Step [100/600], Loss: 0.1369\n",
            "Epoch [2/5], Step [200/600], Loss: 0.1461\n",
            "Epoch [2/5], Step [300/600], Loss: 0.0674\n",
            "Epoch [2/5], Step [400/600], Loss: 0.0785\n",
            "Epoch [2/5], Step [500/600], Loss: 0.0352\n",
            "Epoch [2/5], Step [600/600], Loss: 0.1625\n",
            "Epoch [3/5], Step [100/600], Loss: 0.0242\n",
            "Epoch [3/5], Step [200/600], Loss: 0.0697\n",
            "Epoch [3/5], Step [300/600], Loss: 0.2257\n",
            "Epoch [3/5], Step [400/600], Loss: 0.1261\n",
            "Epoch [3/5], Step [500/600], Loss: 0.1763\n",
            "Epoch [3/5], Step [600/600], Loss: 0.0440\n",
            "Epoch [4/5], Step [100/600], Loss: 0.0742\n",
            "Epoch [4/5], Step [200/600], Loss: 0.0387\n",
            "Epoch [4/5], Step [300/600], Loss: 0.1057\n",
            "Epoch [4/5], Step [400/600], Loss: 0.0249\n",
            "Epoch [4/5], Step [500/600], Loss: 0.0534\n",
            "Epoch [4/5], Step [600/600], Loss: 0.0379\n",
            "Epoch [5/5], Step [100/600], Loss: 0.0238\n",
            "Epoch [5/5], Step [200/600], Loss: 0.0518\n",
            "Epoch [5/5], Step [300/600], Loss: 0.0205\n",
            "Epoch [5/5], Step [400/600], Loss: 0.0530\n",
            "Epoch [5/5], Step [500/600], Loss: 0.0465\n",
            "Epoch [5/5], Step [600/600], Loss: 0.0280\n"
          ]
        }
      ],
      "source": [
        "#@title Training the model\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  for i ,(images,labels) in enumerate(train_gen):\n",
        "    images = Variable(images.view(-1,28*28))\n",
        "    labels = Variable(labels)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    outputs = net(images)\n",
        "    loss = loss_function(outputs, labels)\n",
        "    loss.backward() #Q2a (15 points): What is the meaning of this step? Please explain the functionality.\n",
        "    optimizer.step() #Q2b (15 points): What is the meaning of this step? Please explain the functionality.\n",
        "\n",
        "    if (i+1) % 100 == 0:\n",
        "      print('Epoch [%d/%d], Step [%d/%d], Loss: %.4f'%(epoch+1, num_epochs, i+1, len(train_data)//batch_size, loss.item()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "loss.backward() - computes gradients of loss function with respect to the model parameters and are later used for updating parameters during training via gradient descent or other similar algorithms.\n",
        "\n",
        "optimizer.step() - This will update the model parameters based on the computed gradients and the chosen optimization algorithm (Adam optimizer in above case). It's essentially performing a gradient descent step to minimize the loss."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pjMFDJwOQohr"
      },
      "source": [
        "#Q3 (20 points): Discuss the results. Is the neural network doing a good job?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DTPvMW5jHB9X",
        "outputId": "5a203e8a-a181-4b89-ac40-1064731cac41"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy of the model: 97.920 %\n"
          ]
        }
      ],
      "source": [
        "#@title Evaluating the accuracy of the model\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "for images,labels in test_gen:\n",
        "  images = Variable(images.view(-1,28*28))\n",
        "  labels = labels\n",
        "\n",
        "  output = net(images)\n",
        "  _, predicted = torch.max(output,1)\n",
        "  correct += (predicted == labels).sum()\n",
        "  total += labels.size(0)\n",
        "\n",
        "print('Accuracy of the model: %.3f %%' %((100*correct)/(total+1)))\n",
        "#Q3 (20 points): How to interpret the results? Is the neural network does a good job?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h9LW7wiy07Dp"
      },
      "source": [
        "accuracy is around 97.3 to 98.3 using above model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zxJmRLF-F_8E"
      },
      "source": [
        "Above is a neural network model with one hidden layer and ReLU activation function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0QrRgqJ6rrGJ",
        "outputId": "234cf12c-65ec-40ef-84bd-fbd69fc9022d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/5], Step [100/600], Loss: 0.3478\n",
            "Epoch [1/5], Step [200/600], Loss: 0.1650\n",
            "Epoch [1/5], Step [300/600], Loss: 0.1820\n",
            "Epoch [1/5], Step [400/600], Loss: 0.0816\n",
            "Epoch [1/5], Step [500/600], Loss: 0.1511\n",
            "Epoch [1/5], Step [600/600], Loss: 0.1417\n",
            "Epoch [2/5], Step [100/600], Loss: 0.0391\n",
            "Epoch [2/5], Step [200/600], Loss: 0.0725\n",
            "Epoch [2/5], Step [300/600], Loss: 0.0300\n",
            "Epoch [2/5], Step [400/600], Loss: 0.1114\n",
            "Epoch [2/5], Step [500/600], Loss: 0.0522\n",
            "Epoch [2/5], Step [600/600], Loss: 0.0707\n",
            "Epoch [3/5], Step [100/600], Loss: 0.0159\n",
            "Epoch [3/5], Step [200/600], Loss: 0.0361\n",
            "Epoch [3/5], Step [300/600], Loss: 0.0106\n",
            "Epoch [3/5], Step [400/600], Loss: 0.0616\n",
            "Epoch [3/5], Step [500/600], Loss: 0.1423\n",
            "Epoch [3/5], Step [600/600], Loss: 0.0725\n",
            "Epoch [4/5], Step [100/600], Loss: 0.0285\n",
            "Epoch [4/5], Step [200/600], Loss: 0.0408\n",
            "Epoch [4/5], Step [300/600], Loss: 0.0471\n",
            "Epoch [4/5], Step [400/600], Loss: 0.0116\n",
            "Epoch [4/5], Step [500/600], Loss: 0.0177\n",
            "Epoch [4/5], Step [600/600], Loss: 0.0930\n",
            "Epoch [5/5], Step [100/600], Loss: 0.0029\n",
            "Epoch [5/5], Step [200/600], Loss: 0.0084\n",
            "Epoch [5/5], Step [300/600], Loss: 0.0094\n",
            "Epoch [5/5], Step [400/600], Loss: 0.0276\n",
            "Epoch [5/5], Step [500/600], Loss: 0.0103\n",
            "Epoch [5/5], Step [600/600], Loss: 0.0742\n",
            "Accuracy of the model: 97.790 %\n"
          ]
        }
      ],
      "source": [
        "def build_model(input_size, hidden_size, num_classes, num_layers, activations):\n",
        "    layers = []\n",
        "    layers.append(nn.Linear(input_size, hidden_size))\n",
        "    for i in range(num_layers):\n",
        "        activation = activations[i] if i < len(activations) else 'relu'\n",
        "        if activation == 'relu':\n",
        "            layers.append(nn.ReLU())\n",
        "        elif activation == 'sigmoid':\n",
        "            layers.append(nn.Sigmoid())\n",
        "        elif activation == 'tanh':\n",
        "            layers.append(nn.Tanh())\n",
        "        elif activation == 'softmax':\n",
        "            layers.append(nn.Softmax())\n",
        "        layers.append(nn.Linear(hidden_size, hidden_size))\n",
        "    layers.append(nn.Linear(hidden_size, num_classes))\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "activations = ['relu', 'relu', 'relu', 'relu']\n",
        "net = build_model(input_size, hidden_size, num_classes, num_layers=1, activations=activations)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  net.cuda()\n",
        "\n",
        "\n",
        "#@title function to evaluate different activation functions and different number of layers\n",
        "\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
        "\n",
        "#@title Training the model\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  for i ,(images,labels) in enumerate(train_gen):\n",
        "    images = Variable(images.view(-1,28*28))\n",
        "    labels = Variable(labels)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    outputs = net(images)\n",
        "    loss = loss_function(outputs, labels)\n",
        "    loss.backward() #Q2a (15 points): What is the meaning of this step? Please explain the functionality.\n",
        "    optimizer.step() #Q2b (15 points): What is the meaning of this step? Please explain the functionality.\n",
        "\n",
        "    if (i+1) % 100 == 0:\n",
        "      print('Epoch [%d/%d], Step [%d/%d], Loss: %.4f'%(epoch+1, num_epochs, i+1, len(train_data)//batch_size, loss.item()))\n",
        "\n",
        "\n",
        "#@title Evaluating the accuracy of the model\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "for images,labels in test_gen:\n",
        "  images = Variable(images.view(-1,28*28))\n",
        "  labels = labels\n",
        "\n",
        "  output = net(images)\n",
        "  _, predicted = torch.max(output,1)\n",
        "  correct += (predicted == labels).sum()\n",
        "  total += labels.size(0)\n",
        "\n",
        "print('Accuracy of the model: %.3f %%' %((100*correct)/(total+1)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HuK1LxoYwEWl"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJkMDBXIwE92"
      },
      "source": [
        "activations = ['relu', 'sigmoid', 'tanh', 'softmax']\n",
        "net = build_model(input_size, hidden_size, num_classes, num_layers=4, activations=activations)\n",
        "Accuracy of the model: 97.120 %"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R9AW5Nzbw02l"
      },
      "source": [
        "activations = ['relu', 'sigmoid', 'tanh', 'softmax']\n",
        "net = build_model(input_size, hidden_size, num_classes, num_layers=3, activations=activations)\n",
        "Accuracy of the model: 97.600 %"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sYuCtbOAxHRH"
      },
      "source": [
        "activations = ['relu', 'relu', 'relu', 'relu']\n",
        "net = build_model(input_size, hidden_size, num_classes, num_layers=3, activations=activations)\n",
        "Accuracy of the model: 97.740 %"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a94CYbMcyH4i"
      },
      "source": [
        "activations = ['relu', 'relu', 'relu', 'relu']\n",
        "net = build_model(input_size, hidden_size, num_classes, num_layers=4, activations=activations)\n",
        "Accuracy of the model: 97.850 %"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V--ebv5f2fu_"
      },
      "source": [
        "activations = ['relu', 'relu', 'relu', 'relu']\n",
        "net = build_model(input_size, hidden_size, num_classes, num_layers=6, activations=activations)\n",
        "accuracy - 97.660%"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NuY1BvSy3rHc"
      },
      "source": [
        "relu with 1 layer; accuracy - 97.680%"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5hTrWlEHm4g"
      },
      "source": [
        "above code checks model for different layers and activation functions. Model can be updated accordingly as below which has 3 layers and 'relu' on each layer. Functions and number of layers can be added accordingly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "AmlkY9Y-3quw"
      },
      "outputs": [],
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_classes):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.fc3 = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.fc1(x)\n",
        "        out = self.relu1(out)\n",
        "        out = self.fc2(out)\n",
        "        out = self.relu2(out)\n",
        "        output = self.fc3(out)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1IEcztf6xyI1",
        "outputId": "f4118483-b6d8-49c7-b20e-1590bbb19718"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/5], Step [100/600], Loss: 0.2079\n",
            "Epoch [1/5], Step [200/600], Loss: 0.0920\n",
            "Epoch [1/5], Step [300/600], Loss: 0.0469\n",
            "Epoch [1/5], Step [400/600], Loss: 0.0560\n",
            "Epoch [1/5], Step [500/600], Loss: 0.0944\n",
            "Epoch [1/5], Step [600/600], Loss: 0.0161\n",
            "Epoch [2/5], Step [100/600], Loss: 0.0877\n",
            "Epoch [2/5], Step [200/600], Loss: 0.0248\n",
            "Epoch [2/5], Step [300/600], Loss: 0.1439\n",
            "Epoch [2/5], Step [400/600], Loss: 0.0775\n",
            "Epoch [2/5], Step [500/600], Loss: 0.0538\n",
            "Epoch [2/5], Step [600/600], Loss: 0.1182\n",
            "Epoch [3/5], Step [100/600], Loss: 0.0519\n",
            "Epoch [3/5], Step [200/600], Loss: 0.0154\n",
            "Epoch [3/5], Step [300/600], Loss: 0.0112\n",
            "Epoch [3/5], Step [400/600], Loss: 0.0476\n",
            "Epoch [3/5], Step [500/600], Loss: 0.0091\n",
            "Epoch [3/5], Step [600/600], Loss: 0.0200\n",
            "Epoch [4/5], Step [100/600], Loss: 0.0204\n",
            "Epoch [4/5], Step [200/600], Loss: 0.0508\n",
            "Epoch [4/5], Step [300/600], Loss: 0.0470\n",
            "Epoch [4/5], Step [400/600], Loss: 0.0452\n",
            "Epoch [4/5], Step [500/600], Loss: 0.0829\n",
            "Epoch [4/5], Step [600/600], Loss: 0.0114\n",
            "Epoch [5/5], Step [100/600], Loss: 0.0075\n",
            "Epoch [5/5], Step [200/600], Loss: 0.0276\n",
            "Epoch [5/5], Step [300/600], Loss: 0.0247\n",
            "Epoch [5/5], Step [400/600], Loss: 0.0457\n",
            "Epoch [5/5], Step [500/600], Loss: 0.0019\n",
            "Epoch [5/5], Step [600/600], Loss: 0.0125\n",
            "Accuracy of the model on the 10000 test images: 99 %\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.datasets as dsets\n",
        "import torchvision.transforms as transforms\n",
        "from torch.autograd import Variable\n",
        "\n",
        "#@title CNN implementation\n",
        "\n",
        "input_size = 784 # img_size = (28,28) ---> 28*28=784 in total\n",
        "num_classes = 10 # number of output classes discrete range [0,9]\n",
        "num_epochs = 5 # number of times which the entire dataset is passed throughout the model\n",
        "batch_size = 100 # the size of input data took for one iteration\n",
        "lr = 1e-3 # size of step\n",
        "\n",
        "#@title Downloading MNIST data\n",
        "\n",
        "train_data = dsets.MNIST(root = './data', train = True,\n",
        "                        transform = transforms.ToTensor(), download = True)\n",
        "\n",
        "test_data = dsets.MNIST(root = './data', train = False,\n",
        "                       transform = transforms.ToTensor())\n",
        "\n",
        "#@title Define model class\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 16, kernel_size=5, stride=1, padding=2)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.conv2 = nn.Conv2d(16, 32, kernel_size=5, stride=1, padding=2)\n",
        "        self.fc1 = nn.Linear(7 * 7 * 32, 128)\n",
        "        self.fc2 = nn.Linear(128, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = self.relu(out)\n",
        "        out = self.maxpool(out)\n",
        "        out = self.conv2(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.maxpool(out)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.fc1(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.fc2(out)\n",
        "        return out\n",
        "\n",
        "#@title CNN implementation\n",
        "\n",
        "net = Net(num_classes)\n",
        "if torch.cuda.is_available():\n",
        "  net.cuda()\n",
        "\n",
        "#@title Define loss-function & optimizer\n",
        "\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
        "\n",
        "#@title Training the model\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_data,\n",
        "                                           batch_size=batch_size,\n",
        "                                           shuffle=True)\n",
        "\n",
        "total_step = len(train_loader)\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        images = Variable(images)\n",
        "        labels = Variable(labels)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = net(images)\n",
        "        loss = loss_function(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if (i+1) % 100 == 0:\n",
        "            print('Epoch [%d/%d], Step [%d/%d], Loss: %.4f'\n",
        "                  %(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
        "\n",
        "#@title Evaluating the accuracy of the model\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_data,\n",
        "                                          batch_size=batch_size,\n",
        "                                          shuffle=False)\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "for images, labels in test_loader:\n",
        "    images = Variable(images)\n",
        "    labels = labels\n",
        "\n",
        "    output = net(images)\n",
        "    _, predicted = torch.max(output.data, 1)\n",
        "    correct += (predicted == labels).sum()\n",
        "    total += labels.size(0)\n",
        "\n",
        "print('Accuracy of the model on the 10000 test images: %d %%' % (100 * correct / total))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qAbaPOSiy8YN"
      },
      "source": [
        "CNN model with 2 convolutional layers; used ReLU activation functions and max pooling layers.\n",
        "initial accuracy - 98%"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OQ9YglqU35pz"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9bgr1uQY35kV",
        "outputId": "0123822f-6954-49a1-b862-0e103a123b1e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training model with activation: relu, pooling: max\n",
            "Epoch [1/5], Step [100/600], Loss: 0.2634\n",
            "Epoch [1/5], Step [200/600], Loss: 0.0862\n",
            "Epoch [1/5], Step [300/600], Loss: 0.0988\n",
            "Epoch [1/5], Step [400/600], Loss: 0.2082\n",
            "Epoch [1/5], Step [500/600], Loss: 0.0707\n",
            "Epoch [1/5], Step [600/600], Loss: 0.0395\n",
            "Epoch [2/5], Step [100/600], Loss: 0.1340\n",
            "Epoch [2/5], Step [200/600], Loss: 0.0217\n",
            "Epoch [2/5], Step [300/600], Loss: 0.0446\n",
            "Epoch [2/5], Step [400/600], Loss: 0.0615\n",
            "Epoch [2/5], Step [500/600], Loss: 0.0481\n",
            "Epoch [2/5], Step [600/600], Loss: 0.0185\n",
            "Epoch [3/5], Step [100/600], Loss: 0.1353\n",
            "Epoch [3/5], Step [200/600], Loss: 0.0249\n",
            "Epoch [3/5], Step [300/600], Loss: 0.0586\n",
            "Epoch [3/5], Step [400/600], Loss: 0.0469\n",
            "Epoch [3/5], Step [500/600], Loss: 0.0870\n",
            "Epoch [3/5], Step [600/600], Loss: 0.0232\n",
            "Epoch [4/5], Step [100/600], Loss: 0.0475\n",
            "Epoch [4/5], Step [200/600], Loss: 0.0792\n",
            "Epoch [4/5], Step [300/600], Loss: 0.0196\n",
            "Epoch [4/5], Step [400/600], Loss: 0.0069\n",
            "Epoch [4/5], Step [500/600], Loss: 0.0340\n",
            "Epoch [4/5], Step [600/600], Loss: 0.0246\n",
            "Epoch [5/5], Step [100/600], Loss: 0.0356\n",
            "Epoch [5/5], Step [200/600], Loss: 0.0452\n",
            "Epoch [5/5], Step [300/600], Loss: 0.0481\n",
            "Epoch [5/5], Step [400/600], Loss: 0.0520\n",
            "Epoch [5/5], Step [500/600], Loss: 0.0056\n",
            "Epoch [5/5], Step [600/600], Loss: 0.0472\n",
            "Accuracy of the model on the 10000 test images with activation relu and pooling max: 99.12999725341797 %\n",
            "Training model with activation: relu, pooling: avg\n",
            "Epoch [1/5], Step [100/600], Loss: 0.2264\n",
            "Epoch [1/5], Step [200/600], Loss: 0.3562\n",
            "Epoch [1/5], Step [300/600], Loss: 0.0640\n",
            "Epoch [1/5], Step [400/600], Loss: 0.1025\n",
            "Epoch [1/5], Step [500/600], Loss: 0.1144\n",
            "Epoch [1/5], Step [600/600], Loss: 0.0838\n",
            "Epoch [2/5], Step [100/600], Loss: 0.0536\n",
            "Epoch [2/5], Step [200/600], Loss: 0.0965\n",
            "Epoch [2/5], Step [300/600], Loss: 0.0533\n",
            "Epoch [2/5], Step [400/600], Loss: 0.0550\n",
            "Epoch [2/5], Step [500/600], Loss: 0.0551\n",
            "Epoch [2/5], Step [600/600], Loss: 0.0597\n",
            "Epoch [3/5], Step [100/600], Loss: 0.0565\n",
            "Epoch [3/5], Step [200/600], Loss: 0.0669\n",
            "Epoch [3/5], Step [300/600], Loss: 0.1003\n",
            "Epoch [3/5], Step [400/600], Loss: 0.0324\n",
            "Epoch [3/5], Step [500/600], Loss: 0.0350\n",
            "Epoch [3/5], Step [600/600], Loss: 0.0916\n",
            "Epoch [4/5], Step [100/600], Loss: 0.0343\n",
            "Epoch [4/5], Step [200/600], Loss: 0.0443\n",
            "Epoch [4/5], Step [300/600], Loss: 0.0388\n",
            "Epoch [4/5], Step [400/600], Loss: 0.0506\n",
            "Epoch [4/5], Step [500/600], Loss: 0.0274\n",
            "Epoch [4/5], Step [600/600], Loss: 0.0947\n",
            "Epoch [5/5], Step [100/600], Loss: 0.0109\n",
            "Epoch [5/5], Step [200/600], Loss: 0.0097\n",
            "Epoch [5/5], Step [300/600], Loss: 0.0064\n",
            "Epoch [5/5], Step [400/600], Loss: 0.0405\n",
            "Epoch [5/5], Step [500/600], Loss: 0.0125\n",
            "Epoch [5/5], Step [600/600], Loss: 0.0127\n",
            "Accuracy of the model on the 10000 test images with activation relu and pooling avg: 98.95999908447266 %\n",
            "Training model with activation: sigmoid, pooling: max\n",
            "Epoch [1/5], Step [100/600], Loss: 2.2943\n",
            "Epoch [1/5], Step [200/600], Loss: 1.9212\n",
            "Epoch [1/5], Step [300/600], Loss: 0.5494\n",
            "Epoch [1/5], Step [400/600], Loss: 0.4647\n",
            "Epoch [1/5], Step [500/600], Loss: 0.4137\n",
            "Epoch [1/5], Step [600/600], Loss: 0.4385\n",
            "Epoch [2/5], Step [100/600], Loss: 0.2556\n",
            "Epoch [2/5], Step [200/600], Loss: 0.4516\n",
            "Epoch [2/5], Step [300/600], Loss: 0.1736\n",
            "Epoch [2/5], Step [400/600], Loss: 0.1035\n",
            "Epoch [2/5], Step [500/600], Loss: 0.1683\n",
            "Epoch [2/5], Step [600/600], Loss: 0.2033\n",
            "Epoch [3/5], Step [100/600], Loss: 0.1520\n",
            "Epoch [3/5], Step [200/600], Loss: 0.1440\n",
            "Epoch [3/5], Step [300/600], Loss: 0.1222\n",
            "Epoch [3/5], Step [400/600], Loss: 0.1249\n",
            "Epoch [3/5], Step [500/600], Loss: 0.0784\n",
            "Epoch [3/5], Step [600/600], Loss: 0.0808\n",
            "Epoch [4/5], Step [100/600], Loss: 0.0681\n",
            "Epoch [4/5], Step [200/600], Loss: 0.0919\n",
            "Epoch [4/5], Step [300/600], Loss: 0.1561\n",
            "Epoch [4/5], Step [400/600], Loss: 0.0501\n",
            "Epoch [4/5], Step [500/600], Loss: 0.1328\n",
            "Epoch [4/5], Step [600/600], Loss: 0.0614\n",
            "Epoch [5/5], Step [100/600], Loss: 0.0346\n",
            "Epoch [5/5], Step [200/600], Loss: 0.1494\n",
            "Epoch [5/5], Step [300/600], Loss: 0.0177\n",
            "Epoch [5/5], Step [400/600], Loss: 0.1604\n",
            "Epoch [5/5], Step [500/600], Loss: 0.0421\n",
            "Epoch [5/5], Step [600/600], Loss: 0.1305\n",
            "Accuracy of the model on the 10000 test images with activation sigmoid and pooling max: 98.23999786376953 %\n",
            "Training model with activation: sigmoid, pooling: avg\n",
            "Epoch [1/5], Step [100/600], Loss: 2.3028\n",
            "Epoch [1/5], Step [200/600], Loss: 2.2841\n",
            "Epoch [1/5], Step [300/600], Loss: 1.0302\n",
            "Epoch [1/5], Step [400/600], Loss: 0.4151\n",
            "Epoch [1/5], Step [500/600], Loss: 0.4250\n",
            "Epoch [1/5], Step [600/600], Loss: 0.3971\n",
            "Epoch [2/5], Step [100/600], Loss: 0.2560\n",
            "Epoch [2/5], Step [200/600], Loss: 0.1762\n",
            "Epoch [2/5], Step [300/600], Loss: 0.2541\n",
            "Epoch [2/5], Step [400/600], Loss: 0.1848\n",
            "Epoch [2/5], Step [500/600], Loss: 0.1346\n",
            "Epoch [2/5], Step [600/600], Loss: 0.2151\n",
            "Epoch [3/5], Step [100/600], Loss: 0.2522\n",
            "Epoch [3/5], Step [200/600], Loss: 0.3217\n",
            "Epoch [3/5], Step [300/600], Loss: 0.1766\n",
            "Epoch [3/5], Step [400/600], Loss: 0.1475\n",
            "Epoch [3/5], Step [500/600], Loss: 0.1381\n",
            "Epoch [3/5], Step [600/600], Loss: 0.1324\n",
            "Epoch [4/5], Step [100/600], Loss: 0.0580\n",
            "Epoch [4/5], Step [200/600], Loss: 0.1158\n",
            "Epoch [4/5], Step [300/600], Loss: 0.0860\n",
            "Epoch [4/5], Step [400/600], Loss: 0.0682\n",
            "Epoch [4/5], Step [500/600], Loss: 0.1718\n",
            "Epoch [4/5], Step [600/600], Loss: 0.1266\n",
            "Epoch [5/5], Step [100/600], Loss: 0.1057\n",
            "Epoch [5/5], Step [200/600], Loss: 0.1326\n",
            "Epoch [5/5], Step [300/600], Loss: 0.1515\n",
            "Epoch [5/5], Step [400/600], Loss: 0.0942\n",
            "Epoch [5/5], Step [500/600], Loss: 0.0973\n",
            "Epoch [5/5], Step [600/600], Loss: 0.1332\n",
            "Accuracy of the model on the 10000 test images with activation sigmoid and pooling avg: 97.56999969482422 %\n",
            "Training model with activation: tanh, pooling: max\n",
            "Epoch [1/5], Step [100/600], Loss: 0.1959\n",
            "Epoch [1/5], Step [200/600], Loss: 0.1846\n",
            "Epoch [1/5], Step [300/600], Loss: 0.1010\n",
            "Epoch [1/5], Step [400/600], Loss: 0.0629\n",
            "Epoch [1/5], Step [500/600], Loss: 0.1392\n",
            "Epoch [1/5], Step [600/600], Loss: 0.0684\n",
            "Epoch [2/5], Step [100/600], Loss: 0.0691\n",
            "Epoch [2/5], Step [200/600], Loss: 0.0167\n",
            "Epoch [2/5], Step [300/600], Loss: 0.0698\n",
            "Epoch [2/5], Step [400/600], Loss: 0.0435\n",
            "Epoch [2/5], Step [500/600], Loss: 0.0316\n",
            "Epoch [2/5], Step [600/600], Loss: 0.0133\n",
            "Epoch [3/5], Step [100/600], Loss: 0.1031\n",
            "Epoch [3/5], Step [200/600], Loss: 0.0263\n",
            "Epoch [3/5], Step [300/600], Loss: 0.0941\n",
            "Epoch [3/5], Step [400/600], Loss: 0.0533\n",
            "Epoch [3/5], Step [500/600], Loss: 0.0136\n",
            "Epoch [3/5], Step [600/600], Loss: 0.0075\n",
            "Epoch [4/5], Step [100/600], Loss: 0.0426\n",
            "Epoch [4/5], Step [200/600], Loss: 0.0065\n",
            "Epoch [4/5], Step [300/600], Loss: 0.0083\n",
            "Epoch [4/5], Step [400/600], Loss: 0.0253\n",
            "Epoch [4/5], Step [500/600], Loss: 0.0063\n",
            "Epoch [4/5], Step [600/600], Loss: 0.0206\n",
            "Epoch [5/5], Step [100/600], Loss: 0.0017\n",
            "Epoch [5/5], Step [200/600], Loss: 0.0127\n",
            "Epoch [5/5], Step [300/600], Loss: 0.0041\n",
            "Epoch [5/5], Step [400/600], Loss: 0.0777\n",
            "Epoch [5/5], Step [500/600], Loss: 0.0227\n",
            "Epoch [5/5], Step [600/600], Loss: 0.0185\n",
            "Accuracy of the model on the 10000 test images with activation tanh and pooling max: 98.91000366210938 %\n",
            "Training model with activation: tanh, pooling: avg\n",
            "Epoch [1/5], Step [100/600], Loss: 0.4282\n",
            "Epoch [1/5], Step [200/600], Loss: 0.3099\n",
            "Epoch [1/5], Step [300/600], Loss: 0.1676\n",
            "Epoch [1/5], Step [400/600], Loss: 0.2217\n",
            "Epoch [1/5], Step [500/600], Loss: 0.1322\n",
            "Epoch [1/5], Step [600/600], Loss: 0.0761\n",
            "Epoch [2/5], Step [100/600], Loss: 0.0480\n",
            "Epoch [2/5], Step [200/600], Loss: 0.1465\n",
            "Epoch [2/5], Step [300/600], Loss: 0.0863\n",
            "Epoch [2/5], Step [400/600], Loss: 0.0267\n",
            "Epoch [2/5], Step [500/600], Loss: 0.2406\n",
            "Epoch [2/5], Step [600/600], Loss: 0.0361\n",
            "Epoch [3/5], Step [100/600], Loss: 0.1034\n",
            "Epoch [3/5], Step [200/600], Loss: 0.0736\n",
            "Epoch [3/5], Step [300/600], Loss: 0.0702\n",
            "Epoch [3/5], Step [400/600], Loss: 0.0766\n",
            "Epoch [3/5], Step [500/600], Loss: 0.0485\n",
            "Epoch [3/5], Step [600/600], Loss: 0.0725\n",
            "Epoch [4/5], Step [100/600], Loss: 0.1222\n",
            "Epoch [4/5], Step [200/600], Loss: 0.0254\n",
            "Epoch [4/5], Step [300/600], Loss: 0.0240\n",
            "Epoch [4/5], Step [400/600], Loss: 0.0674\n",
            "Epoch [4/5], Step [500/600], Loss: 0.0770\n",
            "Epoch [4/5], Step [600/600], Loss: 0.0654\n",
            "Epoch [5/5], Step [100/600], Loss: 0.0059\n",
            "Epoch [5/5], Step [200/600], Loss: 0.0170\n",
            "Epoch [5/5], Step [300/600], Loss: 0.0452\n",
            "Epoch [5/5], Step [400/600], Loss: 0.0813\n",
            "Epoch [5/5], Step [500/600], Loss: 0.0240\n",
            "Epoch [5/5], Step [600/600], Loss: 0.0472\n",
            "Accuracy of the model on the 10000 test images with activation tanh and pooling avg: 98.58999633789062 %\n",
            "Training model with activation: softmax, pooling: max\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1511: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/5], Step [100/600], Loss: 2.3073\n",
            "Epoch [1/5], Step [200/600], Loss: 2.1499\n",
            "Epoch [1/5], Step [300/600], Loss: 1.9291\n",
            "Epoch [1/5], Step [400/600], Loss: 1.7264\n",
            "Epoch [1/5], Step [500/600], Loss: 1.6367\n",
            "Epoch [1/5], Step [600/600], Loss: 1.5525\n",
            "Epoch [2/5], Step [100/600], Loss: 1.3800\n",
            "Epoch [2/5], Step [200/600], Loss: 1.3266\n",
            "Epoch [2/5], Step [300/600], Loss: 1.2881\n",
            "Epoch [2/5], Step [400/600], Loss: 1.2172\n",
            "Epoch [2/5], Step [500/600], Loss: 1.2184\n",
            "Epoch [2/5], Step [600/600], Loss: 1.0739\n",
            "Epoch [3/5], Step [100/600], Loss: 1.1657\n",
            "Epoch [3/5], Step [200/600], Loss: 1.1278\n",
            "Epoch [3/5], Step [300/600], Loss: 1.1744\n",
            "Epoch [3/5], Step [400/600], Loss: 1.1106\n",
            "Epoch [3/5], Step [500/600], Loss: 1.1071\n",
            "Epoch [3/5], Step [600/600], Loss: 1.0743\n",
            "Epoch [4/5], Step [100/600], Loss: 0.9926\n",
            "Epoch [4/5], Step [200/600], Loss: 0.9965\n",
            "Epoch [4/5], Step [300/600], Loss: 1.0429\n",
            "Epoch [4/5], Step [400/600], Loss: 0.9816\n",
            "Epoch [4/5], Step [500/600], Loss: 0.9818\n",
            "Epoch [4/5], Step [600/600], Loss: 0.9236\n",
            "Epoch [5/5], Step [100/600], Loss: 0.9665\n",
            "Epoch [5/5], Step [200/600], Loss: 0.9473\n",
            "Epoch [5/5], Step [300/600], Loss: 0.9554\n",
            "Epoch [5/5], Step [400/600], Loss: 0.9297\n",
            "Epoch [5/5], Step [500/600], Loss: 0.8637\n",
            "Epoch [5/5], Step [600/600], Loss: 0.8805\n",
            "Accuracy of the model on the 10000 test images with activation softmax and pooling max: 50.75 %\n",
            "Training model with activation: softmax, pooling: avg\n",
            "Epoch [1/5], Step [100/600], Loss: 2.3030\n",
            "Epoch [1/5], Step [200/600], Loss: 2.1887\n",
            "Epoch [1/5], Step [300/600], Loss: 1.9355\n",
            "Epoch [1/5], Step [400/600], Loss: 1.7694\n",
            "Epoch [1/5], Step [500/600], Loss: 1.6624\n",
            "Epoch [1/5], Step [600/600], Loss: 1.5159\n",
            "Epoch [2/5], Step [100/600], Loss: 1.4562\n",
            "Epoch [2/5], Step [200/600], Loss: 1.3378\n",
            "Epoch [2/5], Step [300/600], Loss: 1.2458\n",
            "Epoch [2/5], Step [400/600], Loss: 1.1820\n",
            "Epoch [2/5], Step [500/600], Loss: 1.1994\n",
            "Epoch [2/5], Step [600/600], Loss: 1.1897\n",
            "Epoch [3/5], Step [100/600], Loss: 1.1513\n",
            "Epoch [3/5], Step [200/600], Loss: 1.1231\n",
            "Epoch [3/5], Step [300/600], Loss: 1.1452\n",
            "Epoch [3/5], Step [400/600], Loss: 1.1741\n",
            "Epoch [3/5], Step [500/600], Loss: 1.1515\n",
            "Epoch [3/5], Step [600/600], Loss: 1.0100\n",
            "Epoch [4/5], Step [100/600], Loss: 0.9501\n",
            "Epoch [4/5], Step [200/600], Loss: 0.9848\n",
            "Epoch [4/5], Step [300/600], Loss: 1.1054\n",
            "Epoch [4/5], Step [400/600], Loss: 0.9482\n",
            "Epoch [4/5], Step [500/600], Loss: 1.0657\n",
            "Epoch [4/5], Step [600/600], Loss: 1.0135\n",
            "Epoch [5/5], Step [100/600], Loss: 0.9469\n",
            "Epoch [5/5], Step [200/600], Loss: 0.8603\n",
            "Epoch [5/5], Step [300/600], Loss: 0.8846\n",
            "Epoch [5/5], Step [400/600], Loss: 0.8474\n",
            "Epoch [5/5], Step [500/600], Loss: 1.0053\n",
            "Epoch [5/5], Step [600/600], Loss: 0.9385\n",
            "Accuracy of the model on the 10000 test images with activation softmax and pooling avg: 50.86000061035156 %\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.datasets as dsets\n",
        "import torchvision.transforms as transforms\n",
        "from torch.autograd import Variable\n",
        "\n",
        "#@title Define Hyperparameters\n",
        "\n",
        "input_size = 784 # img_size = (28,28) ---> 28*28=784 in total\n",
        "num_classes = 10 # number of output classes discrete range [0,9]\n",
        "num_epochs = 5 # number of times which the entire dataset is passed throughout the model\n",
        "batch_size = 100 # the size of input data took for one iteration\n",
        "lr = 1e-3 # size of step\n",
        "\n",
        "#@title Downloading MNIST data\n",
        "\n",
        "train_data = dsets.MNIST(root = './data', train = True,\n",
        "                        transform = transforms.ToTensor(), download = True)\n",
        "\n",
        "test_data = dsets.MNIST(root = './data', train = False,\n",
        "                       transform = transforms.ToTensor())\n",
        "\n",
        "#@title Define model class\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self, num_classes, activation='relu', pooling='max'):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 16, kernel_size=5, stride=1, padding=2)\n",
        "        if activation == 'relu':\n",
        "            self.activation = nn.ReLU()\n",
        "        elif activation == 'sigmoid':\n",
        "            self.activation = nn.Sigmoid()\n",
        "        elif activation == 'tanh':\n",
        "            self.activation = nn.Tanh()\n",
        "        elif activation == 'softmax':\n",
        "            self.activation = nn.Softmax()\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2) if pooling == 'max' else nn.AvgPool2d(kernel_size=2, stride=2)\n",
        "        self.conv2 = nn.Conv2d(16, 32, kernel_size=5, stride=1, padding=2)\n",
        "        self.fc1 = nn.Linear(7 * 7 * 32, 128)\n",
        "        self.fc2 = nn.Linear(128, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = self.activation(out)\n",
        "        out = self.maxpool(out)\n",
        "        out = self.conv2(out)\n",
        "        out = self.activation(out)\n",
        "        out = self.maxpool(out)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.fc1(out)\n",
        "        out = self.activation(out)\n",
        "        out = self.fc2(out)\n",
        "        return out\n",
        "\n",
        "#@title Build and Train the model\n",
        "\n",
        "def train_model(activation, pooling):\n",
        "    net = Net(num_classes, activation, pooling)\n",
        "    if torch.cuda.is_available():\n",
        "        net.cuda()\n",
        "\n",
        "    loss_function = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(dataset=train_data,\n",
        "                                               batch_size=batch_size,\n",
        "                                               shuffle=True)\n",
        "\n",
        "    total_step = len(train_loader)\n",
        "    for epoch in range(num_epochs):\n",
        "        for i, (images, labels) in enumerate(train_loader):\n",
        "            images = Variable(images)\n",
        "            labels = Variable(labels)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = net(images)\n",
        "            loss = loss_function(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            if (i+1) % 100 == 0:\n",
        "                print('Epoch [%d/%d], Step [%d/%d], Loss: %.4f'\n",
        "                      %(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
        "\n",
        "    test_loader = torch.utils.data.DataLoader(dataset=test_data,\n",
        "                                              batch_size=batch_size,\n",
        "                                              shuffle=False)\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in test_loader:\n",
        "        images = Variable(images)\n",
        "        labels = labels\n",
        "\n",
        "        output = net(images)\n",
        "        _, predicted = torch.max(output.data, 1)\n",
        "        correct += (predicted == labels).sum()\n",
        "        total += labels.size(0)\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    print('Accuracy of the model on the 10000 test images with activation {} and pooling {}: {} %'.format(activation, pooling, accuracy))\n",
        "\n",
        "# Try different combinations of activation functions and pooling layers\n",
        "activations = ['relu', 'sigmoid', 'tanh', 'softmax']\n",
        "poolings = ['max', 'avg']\n",
        "\n",
        "for activation in activations:\n",
        "    for pooling in poolings:\n",
        "        print(\"Training model with activation: {}, pooling: {}\".format(activation, pooling))\n",
        "        train_model(activation, pooling)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ki1_SNi7A1gX"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dBfnAW7wGRUv"
      },
      "source": [
        "only thing to be modified is modeland based on above values, ReLu activation function with max pooling worked best. If needed, changes can be made accordingly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "osDroj_IA1d_"
      },
      "outputs": [],
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 16, kernel_size=5, stride=1, padding=2)\n",
        "        self.relu = nn.ReLU()\n",
        "        # changes for different activation functions accordingly; nn.Softmax() - softmax function\n",
        "        # nn.Sigmoid() -- sigmod function; nn.Tanh() -- hyperbolic tangent function\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        #self.maxpool = nn.AvgPool2d(kernel_size=2, stride=2)  -- average pooling\n",
        "        self.conv2 = nn.Conv2d(16, 32, kernel_size=5, stride=1, padding=2)\n",
        "        self.fc1 = nn.Linear(7 * 7 * 32, 128)\n",
        "        self.fc2 = nn.Linear(128, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = self.relu(out)\n",
        "        out = self.maxpool(out)\n",
        "        out = self.conv2(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.maxpool(out)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.fc1(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.fc2(out)\n",
        "        return out"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
